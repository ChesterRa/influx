#!/usr/bin/env python3
"""
Scaling Integration Script - Merge New Batches into Main Dataset
Integrates m11 (Tech Infrastructure) and m09 (AI Founders) batches
Strategic Option 1 Execution: Focused Infrastructure Expansion
"""

import json
import hashlib
from datetime import datetime, timezone


def load_main_dataset():
    """Load main dataset"""
    main_authors = {}

    with open("data/latest/latest.jsonl", "r") as f:
        for line in f:
            if line.strip():
                record = json.loads(line)
                handle = record["handle"]
                main_authors[handle] = record

    print(f"Loaded {len(main_authors)} authors from main dataset")
    return main_authors


def load_new_batch(filename, batch_name):
    """Load new batch authors"""
    batch_authors = {}

    try:
        with open(filename, "r") as f:
            for line in f:
                if line.strip():
                    record = json.loads(line)
                    handle = record["handle"]
                    batch_authors[handle] = record

        print(f"Loaded {len(batch_authors)} authors from {batch_name}")
        return batch_authors
    except FileNotFoundError:
        print(f"‚ö†Ô∏è  {batch_name} file not found: {filename}")
        return {}


def integrate_new_authors(main_authors, new_authors, batch_name):
    """Integrate new authors into main dataset"""
    added_count = 0
    duplicate_count = 0
    improvements = []

    for handle, new_record in new_authors.items():
        if handle not in main_authors:
            # Add new author
            main_authors[handle] = new_record
            added_count += 1

            score = new_record.get("meta", {}).get("score", 0)
            followers = new_record.get("followers_count", 0)
            name = new_record.get("name", "Unknown")

            improvements.append(
                {
                    "handle": handle,
                    "name": name,
                    "followers": followers,
                    "score": score,
                    "source": batch_name,
                }
            )
        else:
            duplicate_count += 1
            print(f"  Duplicate: {handle} (already in main dataset)")

    print(f"  Added: {added_count} authors")
    print(f"  Duplicates: {duplicate_count} authors")

    return improvements


def save_integrated_dataset(main_authors, filename):
    """Save integrated dataset"""
    # Sort by score desc, followers desc, handle asc
    sorted_authors = sorted(
        main_authors.values(),
        key=lambda x: (-x["meta"]["score"], -x["followers_count"], x["handle"]),
    )

    with open(filename, "w") as f:
        for author in sorted_authors:
            f.write(json.dumps(author) + "\n")

    print(f"Saved integrated dataset to {filename}")
    return len(sorted_authors)


def update_manifest(count, batch_improvements, total_new_authors):
    """Update manifest with scaling integration details"""
    manifest = {
        "schema_version": "1.0.0",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "count": count,
        "sha256": "",  # Will be calculated after file save
        "source_file": "latest.jsonl",
        "sort_order": "score desc, followers_count desc, handle asc",
        "score_version": "M2_activity_quality_relevance",
        "score_formula": "activity(30%) + quality(50%) + relevance(20%) with 30d metrics",
        "score_note": "M2 full scoring model with real Twitter API metrics integrated - strategic scaling complete",
        "stage": "strategic_scaling_option1_complete",
        "scaling_integration": {
            "strategy": "Option 1 - Focused Infrastructure Expansion",
            "batches_processed": ["m11_tech_infra", "m09_ai_founders"],
            "total_new_authors": total_new_authors,
            "batch_improvements": batch_improvements,
        },
        "key_achievement": f"STRATEGIC SCALING OPTION 1 COMPLETE - Added {total_new_authors} high-quality authors through focused infrastructure expansion. Tech Infrastructure and AI Founders domains successfully integrated with M2 multi-dimensional scoring. Network enhanced with critical infrastructure voices and executive leadership coverage.",
    }

    with open("data/latest/manifest.json", "w") as f:
        json.dump(manifest, f, indent=2)

    return manifest


def main():
    """Execute Strategic Scaling Option 1"""
    print("üöÄ Strategic Scaling Option 1 - Focused Infrastructure Expansion")
    print("=" * 65)

    # Load main dataset
    main_authors = load_main_dataset()
    original_count = len(main_authors)

    # Load new batches
    m11_authors = load_new_batch(
        "m11_tech_infra_scored.jsonl", "M11 Tech Infrastructure"
    )
    m09_authors = load_new_batch("m09_ai_founders_scored.jsonl", "M09 AI Founders")

    # Integrate new authors
    all_improvements = []

    if m11_authors:
        print(f"\nüì¶ Integrating M11 Tech Infrastructure authors...")
        m11_improvements = integrate_new_authors(
            main_authors, m11_authors, "M11 Tech Infrastructure"
        )
        all_improvements.extend(m11_improvements)

    if m09_authors:
        print(f"\nüì¶ Integrating M09 AI Founders authors...")
        m09_improvements = integrate_new_authors(
            main_authors, m09_authors, "M09 AI Founders"
        )
        all_improvements.extend(m09_improvements)

    if not all_improvements:
        print("‚ùå No new authors to integrate - check batch processing")
        return

    # Save integrated dataset
    output_file = "data/latest/latest_scaled.jsonl"
    final_count = save_integrated_dataset(main_authors, output_file)

    # Update manifest
    manifest = update_manifest(final_count, all_improvements, len(all_improvements))

    # Show results
    new_authors_count = final_count - original_count
    print(f"\nüìä Strategic Scaling Results:")
    print(f"  Original authors: {original_count}")
    print(f"  New authors added: {new_authors_count}")
    print(f"  Final total: {final_count}")
    print(f"  Network growth: {new_authors_count / original_count * 100:.1f}%")

    if all_improvements:
        print(f"\nüèÜ Top 10 New Authors Added:")
        # Sort by score then followers
        top_authors = sorted(
            all_improvements, key=lambda x: (-x["score"], -x["followers"])
        )[:10]
        for i, imp in enumerate(top_authors, 1):
            print(
                f"  {i:2d}. {imp['name']} (@{imp['handle']}): {imp['followers']:,} followers ‚Üí {imp['score']:.1f} points [{imp['source']}]"
            )

    print(f"\n‚úÖ Strategic Scaling Option 1 Complete!")
    print(f"üìÅ Enhanced dataset: {output_file}")
    print(f"üìã Manifest updated: data/latest/manifest.json")
    print(f"üéØ Network enhanced with focused infrastructure expansion")

    # Validation
    print(f"\nüîç Validating scaled dataset...")
    import subprocess

    result = subprocess.run(
        [
            "python3",
            "tools/influx-validate",
            "-s",
            "schema/bigv.schema.json",
            output_file,
        ],
        capture_output=True,
        text=True,
    )

    if result.returncode == 0:
        print(f"‚úÖ Validation passed - scaled dataset ready for production")
    else:
        print(f"‚ö†Ô∏è  Validation warnings:")
        print(result.stdout)


if __name__ == "__main__":
    main()
