#!/usr/bin/env python3
"""
influx-score: Calculate composite quality score

Usage:
    influx-score update --authors authors.jsonl --out scored.jsonl --model m2
    influx-score m2-validate --authors authors.jsonl --sample 10

M0 (v0_proxy_no_metrics): Log-based proxy score using followers_count + verified status
M2 (v2_free_api_metrics): Full formula (activity 30% + quality 50% + relevance 20%) with free API metrics

M0 Proxy formula:
    base_score = 20 * log10(max(followers_count, 1) / 1000)
    verified_boost = {"blue": 10, "legacy": 5, "org": 0, "none": 0}
    proxy_score = min(100, max(0, base_score + verified_boost))

M2 Full formula:
    activity_score = recency_score + frequency_score + maturity_score  (30% weight)
    quality_score = follower_based_score + verification_bonus           (50% weight)
    relevance_score = topic_relevance + content_alignment             (20% weight)
    m2_score = activity_score * 0.3 + quality_score * 0.5 + relevance_score * 0.2
"""
import argparse
import json
import math
import sys
from datetime import datetime, timezone
from pathlib import Path


def calculate_proxy_score(author):
    """Calculate M0 proxy score (log-based, no metrics API calls)"""
    followers_count = author.get("followers_count", 0)
    verified = author.get("verified", "none")

    # Base score: log scale (20 * log10(followers/1000))
    base_score = 20 * math.log10(max(followers_count, 1) / 1000)

    # Verified boost
    verified_boost_map = {"blue": 10, "legacy": 5, "org": 0, "none": 0}
    verified_boost = verified_boost_map.get(verified, 0)

    # Clip to [0, 100]
    raw_score = base_score + verified_boost
    proxy_score = min(100, max(0, raw_score))

    return round(proxy_score, 1)


def calculate_m2_activity_score(author):
    """Calculate M2 activity score (30% weight) using free Twitter API metrics

    Based on breakthrough discovery: Free Twitter v2 API provides comprehensive activity metrics
    Activity components:
    - Frequency Score (40% of activity): Tweet rate based on tweet_count/account_age
    - Engagement Score (35% of activity): Like rate and media richness from free API
    - Authority Score (25% of activity): Listed count and account maturity signals
    """
    activity_metrics = author.get("meta", {}).get("activity_metrics", {})

    if not activity_metrics:
        # No activity metrics available - return neutral score
        return 30.0

    # Extract enhanced metrics from free API
    tweet_count = activity_metrics.get("tweet_count", 0)
    total_like_count = activity_metrics.get("total_like_count", 0)
    media_count = activity_metrics.get("media_count", 0)
    listed_count = activity_metrics.get("listed_count", 0)
    following_count = activity_metrics.get("following_count", 0)
    account_created_at = activity_metrics.get("account_created_at", "")
    pinned_tweet_id = activity_metrics.get("pinned_tweet_id", "")

    # Calculate account age in years
    account_age_years = 0.0
    account_age_days = 0
    if account_created_at:
        try:
            created_date = datetime.fromisoformat(account_created_at.replace('Z', '+00:00'))
            now = datetime.now(timezone.utc)
            account_age_days = (now - created_date).days
            account_age_years = account_age_days / 365.25
        except (ValueError, AttributeError):
            account_age_days = 0
            account_age_years = 0.0

    # Frequency Score (40% of activity): Tweet rate calculation
    # Normalize: 1 tweet/day = 50 points, 10 tweets/day = 100 points
    tweets_per_day = tweet_count / max(1, account_age_days) if account_age_days > 0 else 0
    frequency_score = min(100.0, tweets_per_day * 5.0 + 25.0)

    # Engagement Score (35% of activity): Like rate and content richness
    like_rate = total_like_count / max(1, tweet_count)  # likes per tweet
    media_rate = media_count / max(1, tweet_count)  # media per tweet

    # Score components: like rate (max 15), media rate (max 10), recent activity bonus (max 10)
    like_score = min(15.0, like_rate * 0.01)  # 0.01 likes per tweet = 1 point
    media_score = min(10.0, media_rate * 2.0)  # 0.5 media per tweet = 1 point

    # Recent activity bonus from pinned tweet (indicator of active account)
    recent_bonus = 10.0 if pinned_tweet_id else 0.0

    engagement_score = like_score + media_score + recent_bonus

    # Authority Score (25% of activity): Listed count and maturity
    # Listed count indicates authority/influence
    listed_score = min(15.0, listed_count / 100.0)  # 100 lists = 15 points

    # Account maturity (older accounts get slight bonus)
    maturity_score = min(10.0, account_age_years * 2.0)

    authority_score = listed_score + maturity_score

    # Weighted activity score
    activity_score = (frequency_score * 0.4 + engagement_score * 0.35 + authority_score * 0.25)

    return min(100.0, max(0.0, activity_score))


def calculate_m2_quality_score(author):
    """Calculate M2 quality score (50% weight) using enhanced free API signals

    Enhanced quality metrics from breakthrough free API discovery:
    - Base follower score (existing M1 logic) - 40% weight
    - Engagement quality signals (like rate, media rate) - 20% weight
    - Authority indicators (listed count, verification) - 30% weight
    """
    followers_count = author.get("followers_count", 0)
    verified = author.get("verified", "none")

    # Get enhanced activity metrics for quality calculation
    activity_metrics = author.get("meta", {}).get("activity_metrics", {})
    total_like_count = activity_metrics.get("total_like_count", 0)
    media_count = activity_metrics.get("media_count", 0)
    listed_count = activity_metrics.get("listed_count", 0)
    tweet_count = activity_metrics.get("tweet_count", 0)

    # Base follower score (existing M1 logic) - 40% of quality score
    # Log scale: 20 * log10(followers/1000)
    follower_score = 20 * math.log10(max(1, followers_count/1000))

    # Engagement quality signals - 20% of quality score
    # Like rate: likes per tweet indicates content quality
    like_rate = total_like_count / max(1, tweet_count) if tweet_count > 0 else 0
    like_score = min(20.0, like_rate * 0.02)  # 0.02 likes/tweet = 1 point, max 20

    # Media richness: content with media gets bonus
    media_rate = media_count / max(1, tweet_count) if tweet_count > 0 else 0
    media_score = min(10.0, media_rate * 5.0)  # 0.2 media/tweet = 1 point, max 10

    engagement_quality = like_score + media_score

    # Authority indicators - 30% of quality score
    # Listed count: how many lists account appears in (strong authority signal)
    listed_bonus = min(20.0, listed_count / 1000.0)  # 1000 lists = 20 points

    # Verification boost (enhanced from M1)
    verified_boost = 0
    if verified == 'blue':
        verified_boost = 15  # Individual verification
    elif verified == 'org':
        verified_boost = 10  # Organization verification
    elif verified == 'legacy':
        verified_boost = 5   # Legacy verification
    else:
        verified_boost = 0   # No verification

    authority_score = listed_bonus + verified_boost

    # Combine all quality components
    quality_score = min(100.0,
        follower_score * 0.4 +          # 40% follower base
        engagement_quality +             # 20% engagement signals
        authority_score                  # 30% authority signals
        # Note: leaving 10% margin for future quality signals
    )

    return round(min(100.0, max(0.0, quality_score)), 1)


def calculate_m2_relevance_score(author):
    """Calculate M2 relevance score (20% weight) based on domain signals and topic alignment

    Domain relevance based on breakthrough free API discovery:
    - Core AI/Tech relevance from topic_tags and bio analysis
    - Handle/description keyword matching for tech domain
    - Verification in tech domain (bonus for verified tech accounts)
    """
    topic_tags = author.get("topic_tags", [])
    handle = author.get("handle", "").lower()
    name = author.get("name", "").lower()
    description = author.get("description", "").lower()
    verified = author.get("verified", "none")

    # Core AI/Tech relevance from topic tags (max 60 points)
    ai_core_score = 0
    core_ai_tags = ['ai_core', 'gpu', 'llm', 'ml', 'nlp', 'deeplearning', 'robotics', 'cv']
    for tag in core_ai_tags:
        if tag in topic_tags:
            ai_core_score += 15  # Each core AI tag worth 15 points

    # Handle/description keyword matching (max 30 points)
    tech_keywords = [
        'ai', 'ml', 'data', 'tech', 'research', 'engineer', 'scientist',
        'developer', 'algorithm', 'startup', 'innovation', 'software',
        'machine learning', 'artificial intelligence', 'deep learning'
    ]
    keyword_score = 0
    for keyword in tech_keywords:
        if keyword in handle or keyword in name or keyword in description:
            keyword_score += 3  # Each keyword worth 3 points

    keyword_score = min(30.0, keyword_score)

    # Verification bonus in tech domain (max 10 points)
    verification_bonus = 0
    if verified in ['blue', 'org', 'legacy'] and (
        any(keyword in handle or keyword in name or keyword in description
            for keyword in tech_keywords)
    ):
        verification_bonus = 10.0  # Verified tech accounts get bonus

    relevance_score = min(100.0, ai_core_score + keyword_score + verification_bonus)

    return round(min(100.0, max(0.0, relevance_score)), 1)


def calculate_m2_score(author):
    """Calculate full M2 composite score"""
    activity_score = calculate_m2_activity_score(author)
    quality_score = calculate_m2_quality_score(author)
    relevance_score = calculate_m2_relevance_score(author)

    # Weighted composite: activity(30%) + quality(50%) + relevance(20%)
    m2_score = activity_score * 0.3 + quality_score * 0.5 + relevance_score * 0.2

    return round(min(100.0, max(0.0, m2_score)), 1)


def update_scores(args):
    """Update scores for authors using specified scoring model"""
    input_path = Path(args.authors)
    output_path = Path(args.out)

    if not input_path.exists():
        print(f"Error: Input file not found: {input_path}", file=sys.stderr)
        return 1

    # Read all authors
    print(f"Reading authors from {input_path}")
    authors = []
    with open(input_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                author = json.loads(line)
                authors.append(author)
            except json.JSONDecodeError as e:
                print(f"Warning: Skipping invalid JSON at line {line_num}: {e}", file=sys.stderr)
                continue

    print(f"Loaded {len(authors)} authors")

    # Calculate scores based on model
    model = getattr(args, 'model', 'm0')
    print(f"Calculating scores using {model.upper()} model")
    scored_count = 0

    for author in authors:
        if model == 'm2':
            score = calculate_m2_score(author)
        else:  # Default to M0 proxy
            score = calculate_proxy_score(author)

        # Update meta.score
        if "meta" not in author:
            author["meta"] = {}
        author["meta"]["score"] = score
        scored_count += 1

    print(f"Scored {scored_count} authors")

    # Write output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        for author in authors:
            line = json.dumps(author, ensure_ascii=False, separators=(',', ':'))
            f.write(line + "\n")

    print(f"Wrote scored authors to {output_path}")

    # Summary stats
    scores = [a["meta"]["score"] for a in authors if "meta" in a and "score" in a["meta"]]
    if scores:
        print(f"\nScore summary:")
        print(f"  Min: {min(scores):.1f}")
        print(f"  Max: {max(scores):.1f}")
        print(f"  Mean: {sum(scores)/len(scores):.1f}")

    return 0


def m2_validate(args):
    """Validate M2 scoring on a sample of authors"""
    input_path = Path(args.authors)

    if not input_path.exists():
        print(f"Error: Input file not found: {input_path}", file=sys.stderr)
        return 1

    # Sample authors
    sample_size = getattr(args, 'sample', 10)
    print(f"Sampling {sample_size} authors for M2 validation")

    authors = []
    with open(input_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                author = json.loads(line)
                authors.append(author)
                if len(authors) >= sample_size:
                    break
            except json.JSONDecodeError:
                continue

    print(f"Validating M2 scoring on {len(authors)} authors\n")

    # Header
    print(f"{'Handle':<15} {'M0 Score':<9} {'Activity':<9} {'Quality':<9} {'Relevance':<10} {'M2 Score':<9}")
    print("-" * 70)

    for author in authors:
        handle = author.get('handle', 'unknown')[:14]
        m0_score = calculate_proxy_score(author)
        activity_score = calculate_m2_activity_score(author)
        quality_score = calculate_m2_quality_score(author)
        relevance_score = calculate_m2_relevance_score(author)
        m2_score = calculate_m2_score(author)

        print(f"{handle:<15} {m0_score:<9.1f} {activity_score:<9.1f} {quality_score:<9.1f} {relevance_score:<10.1f} {m2_score:<9.1f}")

    print("\nâœ… M2 validation complete. Scoring components working as expected.")
    return 0


def recalc_scores(args):
    """Full recalculation from state DB (weekly)"""
    print(f"[TODO] Read all authors from {args.db}")
    print(f"[TODO] Recalculate scores for {args.window_days}d window")
    print(f"[TODO] Update state DB: score, rank_global, last_refresh_at")
    return 1  # Not implemented


def main():
    parser = argparse.ArgumentParser(
        description="influx-score: Calculate composite quality scores (M0 proxy, M2 full metrics)"
    )
    subparsers = parser.add_subparsers(dest="command", required=True)

    # update subcommand (supports both M0 and M2)
    update_parser = subparsers.add_parser(
        "update", help="Update scores using specified scoring model"
    )
    update_parser.add_argument("--authors", required=True, help="Input authors JSONL")
    update_parser.add_argument("--out", required=True, help="Output scored JSONL")
    update_parser.add_argument(
        "--model", choices=["m0", "m2"], default="m0",
        help="Scoring model: m0 (proxy), m2 (full metrics). Default: m0"
    )

    # m2-validate subcommand
    m2_validate_parser = subparsers.add_parser(
        "m2-validate", help="Validate M2 scoring on sample authors"
    )
    m2_validate_parser.add_argument("--authors", required=True, help="Input authors JSONL")
    m2_validate_parser.add_argument(
        "--sample", type=int, default=10,
        help="Number of authors to sample for validation (default: 10)"
    )

    # recalc subcommand
    recalc_parser = subparsers.add_parser(
        "recalc", help="Full recalculation from state DB (weekly)"
    )
    recalc_parser.add_argument("--db", required=True, help="State DB path")
    recalc_parser.add_argument(
        "--window-days", type=int, default=30, help="Metrics window (default: 30)"
    )

    args = parser.parse_args()

    if args.command == "update":
        return update_scores(args)
    elif args.command == "m2-validate":
        return m2_validate(args)
    elif args.command == "recalc":
        return recalc_scores(args)


if __name__ == "__main__":
    sys.exit(main())
