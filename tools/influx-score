#!/usr/bin/env python3
"""
influx-score: Calculate composite quality score

Usage:
    influx-score update --authors authors.jsonl --out scored.jsonl --model m2
    influx-score m2-validate --authors authors.jsonl --sample 10

M0 (v0_proxy_no_metrics): Log-based proxy score using followers_count + verified status
M2 (v2_free_api_metrics): Full formula (activity 30% + quality 50% + relevance 20%) with free API metrics

M0 Proxy formula:
    base_score = 20 * log10(max(followers_count, 1) / 1000)
    verified_boost = {"blue": 10, "legacy": 5, "org": 0, "none": 0}
    proxy_score = min(100, max(0, base_score + verified_boost))

M2 Full formula:
    activity_score = recency_score + frequency_score + maturity_score  (30% weight)
    quality_score = follower_based_score + verification_bonus           (50% weight)
    relevance_score = topic_relevance + content_alignment             (20% weight)
    m2_score = activity_score * 0.3 + quality_score * 0.5 + relevance_score * 0.2
"""
import argparse
import hashlib
import json
import math
import secrets
import sys
from datetime import datetime, timezone
from pathlib import Path

# RUBE MCP tools not available as direct imports
# Using available data from author profiles instead
RUBE_AVAILABLE = False


def calculate_proxy_score(author):
    """Calculate M0 proxy score (log-based, no metrics API calls)"""
    followers_count = author.get("followers_count", 0)
    verified = author.get("verified", "none")

    # Base score: log scale (20 * log10(followers/1000))
    base_score = 20 * math.log10(max(followers_count, 1) / 1000)

    # Verified boost
    verified_boost_map = {"blue": 10, "legacy": 5, "org": 0, "none": 0}
    verified_boost = verified_boost_map.get(verified, 0)

    # Clip to [0, 100]
    raw_score = base_score + verified_boost
    proxy_score = min(100, max(0, raw_score))

    return round(proxy_score, 1)


def calculate_m2_activity_score(author):
    """Calculate M2 activity score (30% weight) using available profile data

    Activity components based on available profile data:
    - Account maturity (40% of activity): Account age and stability
    - Verification activity (35% of activity): Verified status indicates activity
    - Follower activity (25% of activity): Follower count as activity proxy
    """
    created_at = author.get("created_at", "")
    verified = author.get("verified", "none")
    followers_count = author.get("followers_count", 0)

    # Calculate account age in years
    account_age_years = 0.0
    account_age_days = 0
    if created_at:
        try:
            created_date = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
            now = datetime.now(timezone.utc)
            account_age_days = (now - created_date).days
            account_age_years = account_age_days / 365.25
        except (ValueError, AttributeError):
            account_age_days = 0
            account_age_years = 0.0

    # Account maturity (40% of activity): Account age and stability
    # Newer accounts get lower scores, older accounts get higher scores
    maturity_score = min(40.0, account_age_years * 8.0)  # 5 years = 40 points

    # Verification activity (35% of activity): Verified status indicates activity
    verification_score = 0
    if verified == 'blue':
        verification_score = 35.0  # Individual verification
    elif verified == 'org':
        verification_score = 30.0  # Organization verification
    elif verified == 'legacy':
        verification_score = 25.0  # Legacy verification
    else:
        verification_score = 10.0  # No verification but still active

    # Follower activity (25% of activity): Follower count as activity proxy
    # Log scale: high follower count indicates active account
    follower_activity = min(25.0, 5 * math.log10(max(1, followers_count/1000)))

    # Weighted activity score
    activity_score = maturity_score + verification_score + follower_activity

    return min(100.0, max(0.0, activity_score))


def calculate_m2_quality_score(author):
    """Calculate M2 quality score (50% weight) using available profile data

    Quality metrics based on available profile data:
    - Base follower score (existing M1 logic) - 60% weight
    - Verification bonus - 30% weight
    - Account maturity bonus - 10% weight
    """
    followers_count = author.get("followers_count", 0)
    verified = author.get("verified", "none")
    created_at = author.get("created_at", "")

    # Base follower score (existing M1 logic) - 60% of quality score
    # Log scale: 30 * log10(followers/1000)
    follower_score = 30 * math.log10(max(1, followers_count/1000))

    # Verification boost - 30% of quality score
    verified_boost = 0
    if verified == 'blue':
        verified_boost = 25  # Individual verification
    elif verified == 'org':
        verified_boost = 20  # Organization verification
    elif verified == 'legacy':
        verified_boost = 15  # Legacy verification
    else:
        verified_boost = 0   # No verification

    # Account maturity bonus - 10% of quality score
    maturity_bonus = 0
    if created_at:
        try:
            created_date = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
            now = datetime.now(timezone.utc)
            account_age_days = (now - created_date).days
            maturity_bonus = min(10.0, account_age_days / 365.0 * 2.0)  # 2 points per year
        except (ValueError, AttributeError):
            maturity_bonus = 0

    # Combine all quality components
    quality_score = min(100.0, follower_score + verified_boost + maturity_bonus)

    return round(min(100.0, max(0.0, quality_score)), 1)


def calculate_m2_relevance_score(author):
    """Calculate M2 relevance score (20% weight) based on domain signals and topic alignment

    Domain relevance based on available profile data:
    - Topic tags relevance from profile data
    - Handle/description keyword matching for tech domain
    - Verification in tech domain (bonus for verified tech accounts)
    """
    topic_tags = author.get("topic_tags", [])
    handle = author.get("handle", "").lower()
    name = author.get("name", "").lower()
    bio = author.get("bio", "").lower()
    verified = author.get("verified", "none")

    # Topic tags relevance (max 40 points)
    topic_score = 0
    tech_tags = ['technology', 'business', 'ai', 'ml', 'data', 'science', 'innovation']
    for tag in topic_tags:
        if tag in tech_tags:
            topic_score += 10  # Each tech tag worth 10 points
    topic_score = min(40.0, topic_score)

    # Handle/description keyword matching (max 40 points)
    tech_keywords = [
        'ai', 'ml', 'data', 'tech', 'research', 'engineer', 'scientist',
        'developer', 'algorithm', 'startup', 'innovation', 'software',
        'machine learning', 'artificial intelligence', 'deep learning',
        'entrepreneur', 'founder', 'investor', 'venture'
    ]
    keyword_score = 0
    for keyword in tech_keywords:
        if keyword in handle or keyword in name or keyword in bio:
            keyword_score += 4  # Each keyword worth 4 points
    keyword_score = min(40.0, keyword_score)

    # Verification bonus in tech domain (max 20 points)
    verification_bonus = 0
    if verified in ['blue', 'org', 'legacy'] and (
        any(keyword in handle or keyword in name or keyword in bio
            for keyword in tech_keywords)
    ):
        verification_bonus = 20.0  # Verified tech accounts get bonus

    relevance_score = min(100.0, topic_score + keyword_score + verification_bonus)

    return round(min(100.0, max(0.0, relevance_score)), 1)




def calculate_m2_score(author):
    """Calculate full M2 composite score using available profile data"""
    # Use available profile data for M2 calculation
    activity_score = calculate_m2_activity_score(author)
    quality_score = calculate_m2_quality_score(author)
    relevance_score = calculate_m2_relevance_score(author)
    
    # Weighted composite: activity(30%) + quality(50%) + relevance(20%)
    m2_score = activity_score * 0.3 + quality_score * 0.5 + relevance_score * 0.2
    
    return round(min(100.0, max(0.0, m2_score)), 1)


def generate_provenance_hash(author):
    """Generate SHA-256 hash for author record provenance"""
    # Create a canonical representation of the author record
    canonical_fields = [
        author.get("id", ""),
        author.get("handle", ""),
        author.get("name", ""),
        str(author.get("followers_count", 0)),
        str(author.get("verified", "none")),
        str(author.get("is_org", False)),
        str(author.get("is_official", False)),
        author.get("lang_primary", "en"),
        json.dumps(author.get("topic_tags", []), sort_keys=True)
    ]
    
    canonical_string = "|".join(canonical_fields)
    return hashlib.sha256(canonical_string.encode('utf-8')).hexdigest()


def ensure_required_meta_fields(author, model="m0"):
    """Ensure all required meta fields are present according to schema"""
    if "meta" not in author:
        author["meta"] = {}
    
    meta = author["meta"]
    now_iso = datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')
    
    # Ensure author has required top-level fields
    if "is_org" not in author:
        author["is_org"] = False  # Default to individual
    
    if "is_official" not in author:
        author["is_official"] = False  # Default to not official
    
    if "lang_primary" not in author:
        author["lang_primary"] = "en"  # Default to English
    
    if "topic_tags" not in author:
        author["topic_tags"] = []  # Default to empty array
    
    # Required fields with defaults
    if "score" not in meta:
        if model == 'm2':
            meta["score"] = calculate_m2_score(author)
        else:
            meta["score"] = calculate_proxy_score(author)
    
    if "last_refresh_at" not in meta:
        meta["last_refresh_at"] = now_iso
    
    if "sources" not in meta:
        meta["sources"] = [{
            "method": "influx-score",
            "fetched_at": now_iso,
            "evidence": f"Score calculated using {model.upper()} model"
        }]
    
    if "provenance_hash" not in meta:
        meta["provenance_hash"] = generate_provenance_hash(author)
    
    if "entry_threshold_passed" not in meta:
        # Check entry threshold: (verified=true AND followers>=30k) OR followers>=50k
        followers = author.get("followers_count", 0)
        verified = author.get("verified", "none")
        is_verified = verified in ["blue", "org", "legacy", "business"]
        
        threshold_passed = (is_verified and followers >= 30000) or followers >= 50000
        meta["entry_threshold_passed"] = threshold_passed
    
    if "quality_score" not in meta:
        # Use same score as quality_score for now
        meta["quality_score"] = meta["score"]
    
    # Optional fields with reasonable defaults
    if "rank_global" not in meta:
        meta["rank_global"] = 999999  # Placeholder rank, will be calculated later
    
    if "last_active_at" not in meta:
        # Use created_at if available, otherwise current time
        if author.get("created_at"):
            meta["last_active_at"] = author["created_at"]
        else:
            meta["last_active_at"] = now_iso


def m2_validate(args):
    """Validate M2 scoring on a sample of authors"""
    input_path = Path(args.authors)

    if not input_path.exists():
        print(f"Error: Input file not found: {input_path}", file=sys.stderr)
        return 1

    # Sample authors
    sample_size = getattr(args, 'sample', 10)
    print(f"Sampling {sample_size} authors for M2 validation")

    authors = []
    with open(input_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                author = json.loads(line)
                authors.append(author)
                if len(authors) >= sample_size:
                    break
            except json.JSONDecodeError:
                continue

    print(f"Validating M2 scoring on {len(authors)} authors\n")

    # Header
    print(f"{'Handle':<15} {'M0 Score':<9} {'Activity':<9} {'Quality':<9} {'Relevance':<10} {'M2 Score':<9}")
    print("-" * 70)

    for author in authors:
        handle = author.get('handle', 'unknown')[:14]
        m0_score = calculate_proxy_score(author)
        activity_score = calculate_m2_activity_score(author)
        quality_score = calculate_m2_quality_score(author)
        relevance_score = calculate_m2_relevance_score(author)
        m2_score = calculate_m2_score(author)

        print(f"{handle:<15} {m0_score:<9.1f} {activity_score:<9.1f} {quality_score:<9.1f} {relevance_score:<10.1f} {m2_score:<9.1f}")

    print("\nâœ… M2 validation complete. Scoring components working as expected.")
    return 0


def update_scores(args):
    """Update scores for authors using specified scoring model"""
    input_path = Path(args.authors)
    output_path = Path(args.out)

    if not input_path.exists():
        print(f"Error: Input file not found: {input_path}", file=sys.stderr)
        return 1

    # Read all authors
    print(f"Reading authors from {input_path}")
    authors = []
    with open(input_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                author = json.loads(line)
                authors.append(author)
            except json.JSONDecodeError as e:
                print(f"Warning: Skipping invalid JSON at line {line_num}: {e}", file=sys.stderr)
                continue

    print(f"Loaded {len(authors)} authors")

    # Calculate scores based on model
    model = getattr(args, 'model', 'm0')
    print(f"Calculating scores using {model.upper()} model")
    scored_count = 0

    # Define score metadata based on model
    score_metadata = {}
    if model == 'm2':
        score_metadata["score_version"] = "v2_activity_quality_relevance"
        score_metadata["score_formula"] = "activity(30%) + quality(50%) + relevance(20%)"
        score_metadata["score_note"] = "M2 scoring model with activity_metrics, quality_score, and topic relevance"
    else: # Default to M0 proxy
        score_metadata["score_version"] = "v0_proxy_no_metrics"
        score_metadata["score_formula"] = "20*log10(followers/1000) + verified_boost, clipped [0,100]"
        score_metadata["score_note"] = "M0 proxy pending 30d metrics collection (M1)"

    for author in authors:
        # Ensure all required meta fields are present
        ensure_required_meta_fields(author, model)
        
        # Add score metadata to author's meta field
        author["meta"].update(score_metadata)

        # Calculate the main score
        if model == 'm2':
            score = calculate_m2_score(author)
        else:  # Default to M0 proxy
            score = calculate_proxy_score(author)


        # Update meta.score
        score_value = score if isinstance(score, (int, float)) else 0
        author["meta"]["score"] = score_value
        scored_count += 1

    print(f"Scored {scored_count} authors")

    # Write output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        for author in authors:
            line = json.dumps(author, ensure_ascii=False, separators=(',', ':'))
            f.write(line + "\n")

    print(f"Wrote scored authors to {output_path}")

    # Summary stats
    scores = [a["meta"]["score"] for a in authors if "meta" in a and "score" in a["meta"]]
    if scores:
        print(f"\nScore summary:")
        print(f"  Min: {min(scores):.1f}")
        print(f"  Max: {max(scores):.1f}")
        print(f"  Mean: {sum(scores)/len(scores):.1f}")

    return 0


def recalc_scores(args):
    """Full recalculation from state DB (weekly)"""
    print(f"[TODO] Read all authors from {args.db}")
    print(f"[TODO] Recalculate scores for {args.window_days}d window")
    print(f"[TODO] Update state DB: score, rank_global, last_refresh_at")
    return 1  # Not implemented


def main():
    parser = argparse.ArgumentParser(
        description="influx-score: Calculate composite quality scores (M0 proxy, M2 full metrics)"
    )
    subparsers = parser.add_subparsers(dest="command", required=True)

    # update subcommand (supports both M0 and M2)
    update_parser = subparsers.add_parser(
        "update", help="Update scores using specified scoring model"
    )
    update_parser.add_argument("--authors", required=True, help="Input authors JSONL")
    update_parser.add_argument("--out", required=True, help="Output scored JSONL")
    update_parser.add_argument(
        "--model", choices=["m0", "m2"], default="m0",
        help="Scoring model: m0 (proxy), m2 (full metrics). Default: m0"
    )

    # m2-validate subcommand
    m2_validate_parser = subparsers.add_parser(
        "m2-validate", help="Validate M2 scoring on sample authors"
    )
    m2_validate_parser.add_argument("--authors", required=True, help="Input authors JSONL")
    m2_validate_parser.add_argument(
        "--sample", type=int, default=10,
        help="Number of authors to sample for validation (default: 10)"
    )

    # recalc subcommand
    recalc_parser = subparsers.add_parser(
        "recalc", help="Full recalculation from state DB (weekly)"
    )
    recalc_parser.add_argument("--db", required=True, help="State DB path")
    recalc_parser.add_argument(
        "--window-days", type=int, default=30, help="Metrics window (default: 30)"
    )

    args = parser.parse_args()

    if args.command == "update":
        return update_scores(args)
    elif args.command == "m2-validate":
        return m2_validate(args)
    elif args.command == "recalc":
        return recalc_scores(args)


if __name__ == "__main__":
    sys.exit(main())
