#!/usr/bin/env python3
"""
influx-batch-harvest: Enhanced batch processing for M1 scaling optimization

Supports 50-100 authors per batch with bulk RUBE MCP processing,
parallel execution, and automated workflow: CSV → Twitter lookup → Harvest → Score → Integrate

Usage:
    influx-batch-harvest bulk --seeds-dir lists/seeds/ --batch-size 75 --parallel-batches 3
    influx-batch-harvest single --csv lists/seeds/m21-batch.csv --out m21_harvested.jsonl
"""
import argparse
import asyncio
import json
import sys
import os
import re
import csv
import hashlib
import yaml
import subprocess
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class BatchProcessor:
    """Enhanced batch processor for M1 scaling optimization"""
    
    def __init__(self, batch_size: int = 75, max_parallel_batches: int = 3):
        self.batch_size = batch_size
        self.max_parallel_batches = max_parallel_batches
        self.stats = {
            'total_handles': 0,
            'total_batches': 0,
            'successful_batches': 0,
            'failed_batches': 0,
            'total_authors_processed': 0,
            'total_authors_passed': 0,
            'processing_time_seconds': 0,
            'authors_per_hour': 0
        }
        
    def load_yaml_rules(self, path: str) -> Dict:
        """Load YAML filter rules from file"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Failed to load {path}: {e}")
            sys.exit(1)
    
    def load_seed_files(self, seeds_dir: str) -> List[Tuple[str, List[str], str]]:
        """Load all seed CSV files from directory
        
        Returns:
            List of (filename, handles, category) tuples
        """
        seed_files = []
        seeds_path = Path(seeds_dir)
        
        if not seeds_path.exists():
            logger.error(f"Seeds directory not found: {seeds_dir}")
            sys.exit(1)
            
        for csv_file in seeds_path.glob("*.csv"):
            if csv_file.name.startswith('.'):
                continue
                
            handles = []
            category = self._extract_category_from_filename(csv_file.stem)
            
            try:
                with open(csv_file, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        handle = row.get('handle', '').strip().lstrip('@')
                        if handle:
                            handles.append(handle)
                            
                if handles:
                    seed_files.append((csv_file.name, handles, category))
                    logger.info(f"Loaded {len(handles)} handles from {csv_file.name}")
                    
            except Exception as e:
                logger.error(f"Failed to read {csv_file}: {e}")
                
        return seed_files
    
    def _extract_category_from_filename(self, filename: str) -> str:
        """Extract category from batch filename (e.g., m21-healthtech -> healthtech)"""
        # Remove prefix like m21-, m22-, etc.
        parts = filename.split('-', 1)
        if len(parts) > 1:
            return parts[1].replace('_', ' ').replace('-', ' ')
        return filename
    
    def create_batches(self, handles: List[str], category: str) -> List[Tuple[List[str], str]]:
        """Split handles into batches of specified size"""
        batches = []
        for i in range(0, len(handles), self.batch_size):
            batch_handles = handles[i:i + self.batch_size]
            batches.append((batch_handles, category))
        return batches
    
    def generate_rube_commands(self, batches: List[Tuple[List[str], str]]) -> List[Dict]:
        """Generate RUBE MCP commands for each batch"""
        commands = []
        for i, (handles, category) in enumerate(batches):
            cmd = {
                'batch_id': f"bulk_{i+1:03d}",
                'handles': handles,
                'category': category,
                'command': f"claude-code exec rube-fetch " + ' '.join([f"@{h}" for h in handles]),
                'output_file': f"bulk_{i+1:03d}_fetched.jsonl"
            }
            commands.append(cmd)
        return commands
    
    def execute_rube_batch(self, batch_cmd: Dict) -> Tuple[bool, str, List[Dict]]:
        """Execute a single RUBE MCP batch lookup
        
        Returns:
            (success, error_message, users_data)
        """
        start_time = time.time()
        logger.info(f"Executing batch {batch_cmd['batch_id']}: {len(batch_cmd['handles'])} handles")
        
        try:
            # Prepare RUBE MCP tool call
            rube_tools = [{
                "tool_slug": "TWITTER_USER_LOOKUP_BY_USERNAMES",
                "arguments": {
                    "usernames": batch_cmd['handles'],
                    "user_fields": [
                        "created_at", "description", "public_metrics", 
                        "verified_type", "username", "name", "profile_image_url"
                    ],
                    "expansions": ["pinned_tweet_id"],
                    "tweet_fields": ["created_at", "public_metrics", "text"]
                }
            }]
            
            # Execute via RUBE MCP
            result = subprocess.run([
                sys.executable, "-c", f"""
import json
import sys
from rube_RUBE_MULTI_EXECUTE_TOOL import rube_RUBE_MULTI_EXECUTE_TOOL

tools = {json.dumps(rube_tools)}
session_id = "bulk_batch_{batch_cmd['batch_id']}"

try:
    result = rube_RUBE_MULTI_EXECUTE_TOOL(
        tools=tools,
        session_id=session_id,
        thought="Bulk user lookup for batch {batch_cmd['batch_id']}",
        sync_response_to_workbench=False,
        memory={{"twitter": ["Bulk batch {batch_cmd['batch_id']} processed"]}},
        current_step="FETCHING_USERS",
        current_step_metric=f"0/{{len(batch_cmd['handles'])}} users",
        next_step="PROCESSING_USERS"
    )
    print(json.dumps(result))
except Exception as e:
    print(json.dumps({{"error": str(e)}}))
    sys.exit(1)
"""
            ], capture_output=True, text=True, timeout=300)  # 5 minute timeout
            
            if result.returncode != 0:
                error_msg = f"RUBE MCP execution failed: {result.stderr}"
                logger.error(error_msg)
                return False, error_msg, []
            
            # Parse response
            try:
                response = json.loads(result.stdout)
                if 'error' in response:
                    return False, response['error'], []
                    
                # Extract user data from response
                users_data = []
                if 'data' in response:
                    for tool_result in response['data']:
                        if 'data' in tool_result and isinstance(tool_result['data'], list):
                            users_data.extend(tool_result['data'])
                
                execution_time = time.time() - start_time
                logger.info(f"Batch {batch_cmd['batch_id']} completed: {len(users_data)} users in {execution_time:.1f}s")
                return True, "", users_data
                
            except json.JSONDecodeError as e:
                error_msg = f"Failed to parse RUBE MCP response: {e}"
                logger.error(error_msg)
                return False, error_msg, []
                
        except subprocess.TimeoutExpired:
            error_msg = f"Batch {batch_cmd['batch_id']} timed out after 5 minutes"
            logger.error(error_msg)
            return False, error_msg, []
            
        except Exception as e:
            error_msg = f"Unexpected error in batch {batch_cmd['batch_id']}: {e}"
            logger.error(error_msg)
            return False, error_msg, []
    
    def process_batch_data(self, users_data: List[Dict], category: str, brand_rules: Dict, risk_rules: Dict) -> List[Dict]:
        """Process raw user data through filters and transform to schema"""
        from influx_harvest import (
            check_brand_heuristics, check_risk_flags, passes_entry_threshold, 
            transform_to_schema, compute_provenance_hash
        )
        
        processed_users = []
        filter_stats = {
            'total': 0,
            'entry_threshold_fail': 0,
            'brand_filtered': 0,
            'risk_filtered': 0,
            'passed': 0
        }
        
        for user in users_data:
            filter_stats['total'] += 1
            
            # Entry threshold check
            if not passes_entry_threshold(user):
                filter_stats['entry_threshold_fail'] += 1
                continue
                
            # Brand heuristics
            is_brand, brand_conf, brand_matches = check_brand_heuristics(user, brand_rules, False)
            if is_brand:
                filter_stats['brand_filtered'] += 1
                logger.debug(f"Brand filtered: @{user.get('username')} (conf={brand_conf:.2f})")
                continue
                
            # Risk flags
            risk_flags, risk_matches = check_risk_flags(user, risk_rules, set())
            if risk_flags:
                filter_stats['risk_filtered'] += 1
                logger.debug(f"Risk filtered: @{user.get('username')} (flags={risk_flags})")
                continue
                
            # Transform to schema
            record = transform_to_schema(user, method='bulk_rube_mcp', category=category,
                                     brand_rules=brand_rules, allow_brands=False)
            processed_users.append(record)
            filter_stats['passed'] += 1
            
        logger.info(f"Filter results: {filter_stats['passed']}/{filter_stats['total']} passed")
        return processed_users
    
    def execute_parallel_batches(self, batches: List[Dict], brand_rules: Dict, risk_rules: Dict) -> List[Dict]:
        """Execute multiple batches in parallel"""
        all_processed_users = []
        
        with ThreadPoolExecutor(max_workers=self.max_parallel_batches) as executor:
            # Submit all batch jobs
            future_to_batch = {
                executor.submit(self.execute_rube_batch, batch): batch 
                for batch in batches
            }
            
            # Process completed batches
            for future in as_completed(future_to_batch):
                batch = future_to_batch[future]
                try:
                    success, error_msg, users_data = future.result()
                    
                    if success:
                        # Process the user data through filters
                        processed_users = self.process_batch_data(
                            users_data, batch['category'], brand_rules, risk_rules
                        )
                        all_processed_users.extend(processed_users)
                        self.stats['successful_batches'] += 1
                        self.stats['total_authors_passed'] += len(processed_users)
                    else:
                        logger.error(f"Batch {batch['batch_id']} failed: {error_msg}")
                        self.stats['failed_batches'] += 1
                        
                    self.stats['total_authors_processed'] += len(users_data)
                    
                except Exception as e:
                    logger.error(f"Exception in batch {batch['batch_id']}: {e}")
                    self.stats['failed_batches'] += 1
        
        return all_processed_users
    
    def run_bulk_processing(self, seeds_dir: str, output_dir: str) -> bool:
        """Run complete bulk processing workflow"""
        start_time = time.time()
        logger.info(f"Starting bulk processing: batch_size={self.batch_size}, parallel={self.max_parallel_batches}")
        
        # Load filter rules
        brand_rules = self.load_yaml_rules("lists/rules/brand_heuristics.yml")
        risk_rules = self.load_yaml_rules("lists/rules/risk_terms.yml")
        
        # Load seed files
        seed_files = self.load_seed_files(seeds_dir)
        if not seed_files:
            logger.error("No seed files found")
            return False
            
        # Create all batches
        all_batches = []
        for filename, handles, category in seed_files:
            batches = self.create_batches(handles, category)
            batch_commands = self.generate_rube_commands(batches)
            all_batches.extend(batch_commands)
            
        self.stats['total_handles'] = sum(len(handles) for _, handles, _ in seed_files)
        self.stats['total_batches'] = len(all_batches)
        
        logger.info(f"Created {len(all_batches)} batches from {len(seed_files)} seed files")
        
        # Execute batches in parallel
        processed_users = self.execute_parallel_batches(all_batches, brand_rules, risk_rules)
        
        # Calculate final stats
        self.stats['processing_time_seconds'] = time.time() - start_time
        self.stats['authors_per_hour'] = (len(processed_users) / self.stats['processing_time_seconds']) * 3600 if self.stats['processing_time_seconds'] > 0 else 0
        
        # Save results
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Save processed users
        results_file = output_path / f"bulk_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
        with open(results_file, 'w', encoding='utf-8') as f:
            for user in processed_users:
                f.write(json.dumps(user, ensure_ascii=False) + '\n')
        
        # Save processing stats
        stats_file = output_path / f"bulk_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, indent=2)
        
        # Log final results
        logger.info(f"Bulk processing completed:")
        logger.info(f"  Total handles: {self.stats['total_handles']}")
        logger.info(f"  Total batches: {self.stats['total_batches']}")
        logger.info(f"  Successful batches: {self.stats['successful_batches']}")
        logger.info(f"  Failed batches: {self.stats['failed_batches']}")
        logger.info(f"  Authors processed: {self.stats['total_authors_processed']}")
        logger.info(f"  Authors passed filters: {self.stats['total_authors_passed']}")
        logger.info(f"  Processing time: {self.stats['processing_time_seconds']:.1f}s")
        logger.info(f"  Throughput: {self.stats['authors_per_hour']:.1f} authors/hour")
        logger.info(f"  Results saved to: {results_file}")
        
        return True


def main():
    parser = argparse.ArgumentParser(
        description="influx-batch-harvest: Enhanced batch processing for M1 scaling"
    )
    subparsers = parser.add_subparsers(dest="command", required=True)
    
    # bulk subcommand
    bulk_parser = subparsers.add_parser("bulk", help="Process all seed files in bulk")
    bulk_parser.add_argument(
        "--seeds-dir", required=True, 
        help="Directory containing seed CSV files"
    )
    bulk_parser.add_argument(
        "--batch-size", type=int, default=75,
        help="Number of handles per batch (default: 75)"
    )
    bulk_parser.add_argument(
        "--parallel-batches", type=int, default=3,
        help="Maximum parallel batches (default: 3)"
    )
    bulk_parser.add_argument(
        "--output-dir", default="archive/bulk_results",
        help="Output directory for results (default: archive/bulk_results)"
    )
    
    # single subcommand
    single_parser = subparsers.add_parser("single", help="Process single CSV file")
    single_parser.add_argument(
        "--csv", required=True, help="Single CSV file to process"
    )
    single_parser.add_argument(
        "--out", required=True, help="Output JSONL file"
    )
    single_parser.add_argument(
        "--batch-size", type=int, default=75,
        help="Number of handles per batch (default: 75)"
    )
    
    args = parser.parse_args()
    
    if args.command == "bulk":
        processor = BatchProcessor(
            batch_size=args.batch_size,
            max_parallel_batches=args.parallel_batches
        )
        success = processor.run_bulk_processing(args.seeds_dir, args.output_dir)
        sys.exit(0 if success else 1)
        
    elif args.command == "single":
        # For single file processing, delegate to existing influx-harvest
        # but with enhanced batching
        logger.info(f"Processing single file: {args.csv}")
        cmd = [
            sys.executable, "tools/influx-harvest", "x-lists",
            "--list-urls", args.csv,
            "--out", args.out,
            "--prefetched-users", f"{args.csv}.fetched.jsonl"  # Expected to be pre-fetched
        ]
        
        result = subprocess.run(cmd)
        sys.exit(result.returncode)


if __name__ == "__main__":
    main()