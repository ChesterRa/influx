#!/usr/bin/env python3
"""
influx-harvest: Author discovery via GitHub seeds and following-graph expansion

Usage:
    influx-harvest github-seeds --orgs openai,anthropic,pytorch --out authors.jsonl
    influx-harvest following --seeds authors.jsonl --pages 2 --out expanded.jsonl
    influx-harvest x-lists --list-urls lists.txt --out curated.jsonl

M0 approach: GitHub org members (twitter_username) + TWITTER_FOLLOWING expansion
M1 approach: Manual CSV + X Lists with brand/risk filtering
"""
import argparse
import sys
import json
import os
import re
import csv
import hashlib
import yaml
import subprocess
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any, Optional


def load_yaml_rules(path: str) -> Dict:
    """Load YAML filter rules from file"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        print(f"[ERROR] Failed to load {path}: {e}", file=sys.stderr)
        sys.exit(1)


def match_keywords(text: str, keywords: List[str], word_boundary: bool = False) -> List[str]:
    """Match keywords in text (case-insensitive)

    Args:
        text: Text to search in
        keywords: List of keywords to match
        word_boundary: If True, match whole words only; else substring match

    Returns:
        List of matched keywords
    """
    if not text:
        return []

    text_lower = text.lower()
    matches = []

    for keyword in keywords:
        keyword_lower = keyword.lower()
        if word_boundary:
            # Word boundary match using regex
            pattern = r'\b' + re.escape(keyword_lower) + r'\b'
            if re.search(pattern, text_lower):
                matches.append(keyword)
        else:
            # Substring match
            if keyword_lower in text_lower:
                matches.append(keyword)

    return matches


def check_brand_heuristics(user: Dict, brand_rules: Dict, allow_brands: bool) -> Tuple[bool, float, List[str]]:
    """Check if user matches brand/official heuristics

    Returns:
        (is_brand, confidence_score, matched_rules)
    """
    if allow_brands:
        return (False, 0.0, [])

    name = user.get('name', '')
    username = user.get('username', '')
    bio = user.get('description', '')
    verified = user.get('verified_type', 'none')
    url = user.get('url', '')

    confidence = 0.0
    matched_rules = []
    weights = brand_rules.get('confidence_weights', {})

    # Check org verification
    if brand_rules.get('verification_rules', {}).get('flag_org_verification', True):
        if verified == 'org':
            confidence += weights.get('org_verification', 1.0)
            matched_rules.append('verification=org')

    # Check name keywords (word boundary)
    name_kw = brand_rules.get('name_keywords', {})
    for category, keywords in name_kw.items():
        matches = match_keywords(name, keywords, word_boundary=True)
        if not matches:
            matches = match_keywords(username, keywords, word_boundary=True)
        if matches:
            confidence += weights.get('name_keyword_match', 0.6)
            matched_rules.append(f'name_kw:{category}:{matches[0]}')
            break  # Count once per category

    # Check bio keywords (substring match)
    bio_kw = brand_rules.get('bio_keywords', {})
    for category, keywords in bio_kw.items():
        matches = match_keywords(bio, keywords, word_boundary=False)
        if matches:
            confidence += weights.get('bio_keyword_match', 0.4)
            matched_rules.append(f'bio_kw:{category}:{matches[0]}')
            break

    # Check domain patterns
    domain_patterns = brand_rules.get('domain_patterns', [])
    for pattern_entry in domain_patterns:
        pattern = pattern_entry.get('pattern', '')
        reason = pattern_entry.get('reason', '')
        exceptions = pattern_entry.get('exceptions', [])

        # Check URL and bio links
        domains_to_check = []
        if url:
            domains_to_check.append(url)
        # Extract URLs from bio
        url_pattern = r'https?://([^\s]+)'
        bio_urls = re.findall(url_pattern, bio)
        domains_to_check.extend(bio_urls)

        for domain in domains_to_check:
            # Check if domain matches pattern and not in exceptions
            if re.search(pattern, domain, re.IGNORECASE):
                is_exception = any(exc in domain for exc in exceptions)
                if not is_exception:
                    confidence += weights.get('domain_match', 0.8)
                    matched_rules.append(f'domain:{reason}')
                    break

        if matched_rules and 'domain:' in matched_rules[-1]:
            break

    # Check exceptions list
    exceptions = brand_rules.get('exceptions', [])
    if username.lower() in [e.lower() for e in exceptions]:
        return (False, 0.0, ['exception_override'])

    # Threshold check
    threshold = weights.get('flag_threshold', 0.7)
    is_brand = confidence >= threshold

    return (is_brand, confidence, matched_rules)


def passes_entry_threshold(user: Dict) -> bool:
    """Check if user passes the entry threshold:
    (verified=true AND followers>=30k) OR followers>=50k

    Args:
        user: User data dictionary

    Returns:
        True if user passes threshold, False otherwise
    """
    verified = user.get('verified_type', 'none')
    followers = user.get('followers_count', 0)

    is_verified = verified in ['blue', 'org', 'legacy']
    verified_min = is_verified and followers >= 30000
    unverified_min = followers >= 50000

    return verified_min or unverified_min


def calculate_provenance_hash(user: Dict) -> str:
    """Calculate SHA-256 hash for provenance tracking

    Args:
        user: User data dictionary

    Returns:
        SHA-256 hex string
    """
    key_fields = f"{user.get('id', '')}{user.get('followers_count', 0)}{user.get('username', '')}{user.get('name', '')}"
    return hashlib.sha256(key_fields.encode()).hexdigest()


def check_risk_flags(user: Dict, risk_rules: Dict, allow_risk: Set[str]) -> Tuple[List[str], List[str]]:
    """Check if user matches risk term patterns

    Returns:
        (risk_flags, matched_rules)
    """
    name = user.get('name', '')
    username = user.get('username', '')
    bio = user.get('description', '')

    risk_flags = []
    matched_rules = []

    # Check each risk category
    risk_categories = ['nsfw', 'political', 'controversy', 'spam', 'hate_speech', 'scam']

    for category in risk_categories:
        if category not in risk_rules:
            continue

        rules = risk_rules[category]
        flag_name = rules.get('flag_name', category)
        auto_exclude = rules.get('auto_exclude', True)

        # Check bio keywords
        bio_kw = rules.get('bio_keywords', [])
        matches = match_keywords(bio, bio_kw, word_boundary=False)
        if matches:
            risk_flags.append(flag_name)
            matched_rules.append(f'{category}:bio:{matches[0]}')
            continue

        # Check name keywords
        name_kw = rules.get('name_keywords', [])
        if name_kw:
            matches = match_keywords(name, name_kw, word_boundary=False)
            if not matches:
                matches = match_keywords(username, name_kw, word_boundary=False)
            if matches:
                risk_flags.append(flag_name)
                matched_rules.append(f'{category}:name:{matches[0]}')
                continue

        # Check name patterns (regex)
        name_patterns = rules.get('name_patterns', [])
        for pattern in name_patterns:
            if re.search(pattern, username, re.IGNORECASE) or re.search(pattern, name, re.IGNORECASE):
                risk_flags.append(flag_name)
                matched_rules.append(f'{category}:name_pattern')
                break

    # Filter out allowed risks
    filtered_flags = [flag for flag in risk_flags if flag not in allow_risk]

    return (filtered_flags, matched_rules)


def compute_provenance_hash(user: Dict) -> str:
    """Compute SHA-256 provenance hash from canonical fields"""
    canonical = {
        'id': user.get('id', ''),
        'username': user.get('username', ''),
        'followers_count': user.get('public_metrics', {}).get('followers_count', 0),
        'verified': user.get('verified_type', 'none'),
    }
    # Sort keys and compact JSON (no spaces)
    canonical_json = json.dumps(canonical, sort_keys=True, separators=(',', ':'))
    return hashlib.sha256(canonical_json.encode('utf-8')).hexdigest()


def transform_to_schema(user: Dict, method: str, category: str = '', brand_rules: Optional[Dict] = None, allow_brands: bool = False) -> Dict:
    """Transform Twitter API user object to influx schema format"""
    now = datetime.now(timezone.utc).isoformat()

    public_metrics = user.get('public_metrics', {})
    verified_type = user.get('verified_type', 'none')

    # Map verified_type to schema format
    if verified_type == 'blue':
        verified = 'blue'
    elif verified_type in ['business', 'government']:
        verified = 'org'
    elif verified_type == 'none':
        verified = 'none'
    else:
        verified = 'legacy' if user.get('verified', False) else 'none'

    # Apply brand heuristics if rules provided
    is_org = False
    if brand_rules:
        is_org, confidence, matches = check_brand_heuristics(user, brand_rules, allow_brands)

    # Implement official detection: government, verified org, or platform accounts
    is_official = (
        verified_type == 'government' or
        verified_type == 'business' or
        (verified_type in ['blue', 'legacy'] and any(keyword in user.get('name', '').lower()
            for keyword in ['official', 'team', 'support', 'admin', 'security']))
    )

    record = {
        'id': user.get('id', ''),
        'handle': user.get('username', ''),
        'name': user.get('name', ''),
        'verified': verified,
        'followers_count': public_metrics.get('followers_count', 0),
        'is_org': is_org,
        'is_official': is_official,
        'lang_primary': 'en',  # Placeholder
        'topic_tags': [category] if category else [],
        'meta': {
            'score': 0.0,  # Placeholder for M1
            'last_refresh_at': now,
            'sources': [{
                'method': method,
                'fetched_at': now,
                'evidence': f"@{user.get('username', '')}"
            }],
            'provenance_hash': compute_provenance_hash(user)
        }
    }

    return record


def github_seeds(args):
    """Fetch GitHub org members with twitter_username field via RUBE MCP"""
    try:
        # Import RUBE MCP tools
        from anthropic import Anthropic

        client = Anthropic()
        session_id = "influx-harvest-github-seeds"

        print(f"[INFO] Fetching GitHub org members from: {args.orgs}", file=sys.stderr)
        org_list = [org.strip() for org in args.orgs.split(',')]

        # Step 1: Search for GitHub org members with twitter_username
        print(f"[INFO] Searching {len(org_list)} orgs for members with twitter_username...", file=sys.stderr)

        twitter_usernames = {}  # {username: {github_login, name, ...}}

        for org in org_list:
            print(f"[INFO] Processing org: {org}", file=sys.stderr)

            # RUBE GITHUB_SEARCH_USERS: org:openai type:user
            # Then for each user: GITHUB_GET_A_USER to get twitter_username field
            # This would be called via RUBE_MULTI_EXECUTE_TOOL

            # For now, return placeholder message since GitHub auth is blocking
            print(f"[BLOCKED] GitHub connection not authorized", file=sys.stderr)
            print(f"[BLOCKED] Cannot execute GITHUB_SEARCH_USERS", file=sys.stderr)
            print(f"[BLOCKED] Authorize GitHub at: https://connect.composio.dev/link/lk_gaA1JjyCCAmW", file=sys.stderr)
            return 1

        # Step 2: Batch verify Twitter usernames via TWITTER_USER_LOOKUP_BY_USERNAMES
        # (This would happen after GitHub data collection)

        # Step 3: Apply brand/risk filters with threshold checking
        filtered_authors = []

        for user_data in github_authors:
            # Convert to standard format
            user = {
                'id': user_data.get('twitter_id', ''),
                'username': user_data.get('twitter_username', ''),
                'name': user_data.get('name', ''),
                'description': user_data.get('bio', ''),
                'followers_count': user_data.get('twitter_followers', 0),
                'verified_type': user_data.get('verified', 'none')
            }

            # Apply entry threshold: (verified=true AND followers>=30k) OR followers>=50k
            if not passes_entry_threshold(user):
                continue

            # Apply brand heuristics
            is_org, confidence, matches = check_brand_heuristics(user, brand_rules, args.allow_brands)

            # Implement official detection: government, verified org, or platform accounts
            is_official = (
                user.get('verified_type') == 'government' or
                user.get('verified_type') == 'business' or
                (user.get('verified_type') in ['blue', 'legacy'] and
                 any(keyword in user.get('name', '').lower()
                     for keyword in ['official', 'team', 'support', 'admin', 'security']))
            )

            # Skip org accounts unless explicitly allowed
            if is_org and not args.allow_brands:
                continue

            # Create final record with all required fields
            record = {
                "id": user['id'],
                "handle": user['username'],
                "name": user['name'],
                "verified": user['verified_type'],
                "followers_count": user['followers_count'],
                "is_org": is_org,
                "is_official": is_official,
                "lang_primary": "en",  # Default - could be detected from bio
                "topic_tags": ["tech"],  # Basic topic detection for github seeds
                "meta": {
                    "score": 0,  # Will be calculated by influx-score
                    "last_refresh_at": datetime.now(timezone.utc).isoformat(),
                    "sources": [{
                        "method": "github_seed",
                        "fetched_at": datetime.now(timezone.utc).isoformat(),
                        "evidence": f"github_org:{args.orgs}"
                    }],
                    "provenance_hash": calculate_provenance_hash(user)
                }
            }

            filtered_authors.append(record)

        # Step 4: Output JSONL with required schema fields
        output_path = Path(args.out)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            for record in filtered_authors:
                f.write(json.dumps(record, separators=(',', ':')) + '\n')

        print(f"[INFO] Wrote {len(filtered_authors)} filtered authors to {args.out}", file=sys.stderr)
        print(f"[INFO] Applied entry threshold and brand heuristics filtering", file=sys.stderr)
        return 0

    except ImportError:
        print("[ERROR] anthropic library not installed. Install with: pip install anthropic", file=sys.stderr)
        return 1
    except Exception as e:
        print(f"[ERROR] {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return 1


def following_expansion(args):
    """Expand via TWITTER_FOLLOWING_BY_USER_ID (who seeds follow)"""
    try:
        # Load filter rules
        brand_rules = load_yaml_rules(args.brand_rules)
        risk_rules = load_yaml_rules(args.risk_rules)
        allow_risk = set(args.allow_risk.split(',')) if args.allow_risk else set()

        print(f"[INFO] Reading seeds from {args.seeds}", file=sys.stderr)

        # Load seed authors
        seed_authors = []
        with open(args.seeds, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    seed_authors.append(json.loads(line))

        print(f"[INFO] Loaded {len(seed_authors)} seed authors", file=sys.stderr)
        print(f"[INFO] For each seed: TWITTER_FOLLOWING_BY_USER_ID, {args.pages} pages max", file=sys.stderr)

        all_authors = {}
        processed_seeds = 0

        # This would need actual TWITTER_FOLLOWING_BY_USER_ID calls via RUBE MCP
        # For now, implement the filtering logic structure
        print(f"[TODO] Execute TWITTER_FOLLOWING_BY_USER_ID calls via RUBE MCP", file=sys.stderr)
        print(f"[TODO] Process {args.pages} pages per seed author", file=sys.stderr)

        # Apply filters: verified + 30k OR 50k followers
        print(f"[INFO] Applying entry threshold and brand/risk filters", file=sys.stderr)

        filtered_authors = []
        for author_id, author_data in all_authors.items():
            # Apply entry threshold
            if not passes_entry_threshold(author_data):
                continue

            # Apply brand heuristics
            is_org, confidence, matches = check_brand_heuristics(author_data, brand_rules, args.allow_brands)

            # Implement official detection: government, verified org, or platform accounts
            is_official = (
                author_data.get('verified_type') == 'government' or
                author_data.get('verified_type') == 'business' or
                (author_data.get('verified_type') in ['blue', 'legacy'] and
                 any(keyword in author_data.get('name', '').lower()
                     for keyword in ['official', 'team', 'support', 'admin', 'security']))
            )

            # Skip org accounts unless explicitly allowed
            if is_org and not args.allow_brands:
                continue

            # Create final record with required fields
            record = {
                "id": author_data['id'],
                "handle": author_data['username'],
                "name": author_data['name'],
                "verified": author_data['verified_type'],
                "followers_count": author_data['followers_count'],
                "is_org": is_org,
                "is_official": is_official,
                "lang_primary": "en",
                "topic_tags": [],
                "meta": {
                    "score": 0,
                    "last_refresh_at": datetime.now(timezone.utc).isoformat(),
                    "sources": [{
                        "method": "following_expansion",
                        "fetched_at": datetime.now(timezone.utc).isoformat(),
                        "evidence": f"following_pages:{args.pages}"
                    }],
                    "provenance_hash": calculate_provenance_hash(author_data)
                }
            }
            filtered_authors.append(record)

        # Dedupe and output
        print(f"[INFO] Deduping {len(filtered_authors)} authors", file=sys.stderr)

        # Write to output
        output_path = Path(args.out)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            for record in filtered_authors:
                f.write(json.dumps(record, separators=(',', ':')) + '\n')

        print(f"[INFO] Wrote {len(filtered_authors)} filtered authors to {args.out}", file=sys.stderr)
        return 0

    except Exception as e:
        print(f"[ERROR] {e}", file=sys.stderr)
        return 1


def x_lists(args):
    """Import curated X Lists (manual CSV one-time)"""
    try:
        # Load filter rules
        brand_rules = load_yaml_rules(args.brand_rules)
        risk_rules = load_yaml_rules(args.risk_rules)
        allow_risk = set(args.allow_risk.split(',')) if args.allow_risk else set()

        print(f"[INFO] Loading handles from {args.list_urls}", file=sys.stderr)

        # Read CSV handles
        handles = []
        categories = {}

        with open(args.list_urls, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                handle = row.get('handle', '').strip().lstrip('@')
                category = row.get('category', '').strip()
                if handle:
                    handles.append(handle)
                    categories[handle] = category

        print(f"[INFO] Loaded {len(handles)} handles from CSV", file=sys.stderr)

        if not handles:
            print("[ERROR] No handles found in CSV", file=sys.stderr)
            return 1

        # Phase 2: Prefer prefetched users, else call bridge to instruct RUBE MCP workflow
        users = []  # type: List[Dict[str, Any]]
        if getattr(args, 'prefetched_users', None):
            src = args.prefetched_users
            print(f"[INFO] Loading prefetched users from {src}", file=sys.stderr)
            count = 0
            with open(src, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        obj = json.loads(line)
                        # Accept either full user object or wrapped {"data": {...}}
                        if isinstance(obj, dict) and 'data' in obj and isinstance(obj['data'], dict):
                            obj = obj['data']
                        if isinstance(obj, dict):
                            users.append(obj)
                            count += 1
                    except json.JSONDecodeError:
                        continue
            print(f"[INFO] Loaded {count} user objects", file=sys.stderr)
        else:
            # Attempt to call the bridge which outputs instructions and exits 2
            bridge_path = str(Path(__file__).with_name('influx-rube-bridge'))
            cmd = [bridge_path, "--handles", ",".join(handles)]
            print(f"[INFO] Looking up Twitter users via bridge (n={len(handles)})", file=sys.stderr)
            try:
                result = subprocess.run(
                    cmd,
                    text=True,
                    capture_output=True,
                    check=False,
                )
            except FileNotFoundError:
                print(f"[ERROR] Bridge not found at {bridge_path}", file=sys.stderr)
                return 1

            if result.returncode == 0:
                # Not expected for the bridge; proceed if any output provided
                if result.stdout.strip():
                    print(result.stdout, file=sys.stderr)
                print("[ERROR] Bridge returned 0 unexpectedly; no users loaded", file=sys.stderr)
                return 1
            elif result.returncode == 2:
                # Friendly workflow instructions
                if result.stdout.strip():
                    print(result.stdout, file=sys.stderr)
                print("[INFO] Re-run x-lists with --prefetched-users once you have JSONL", file=sys.stderr)
                return 2
            else:
                if result.stdout.strip():
                    print(result.stdout, file=sys.stderr)
                if result.stderr.strip():
                    print(result.stderr, file=sys.stderr)
                print(f"[ERROR] Bridge failed with exit code {result.returncode}", file=sys.stderr)
                return result.returncode

        # Apply filters
        print(f"[INFO] Applying filters...", file=sys.stderr)
        filtered_users = []
        filter_stats = {
            'total': 0,
            'entry_threshold_fail': 0,
            'brand_filtered': 0,
            'risk_filtered': 0,
            'passed': 0
        }

        for user in users:
            filter_stats['total'] += 1

            # Entry threshold: (verified AND followers>=30k) OR followers>=50k
            followers = user.get('public_metrics', {}).get('followers_count', 0)
            verified = user.get('verified_type', 'none') != 'none'

            entry_pass = (verified and followers >= args.verified_min_followers) or (followers >= args.min_followers)

            if not entry_pass:
                filter_stats['entry_threshold_fail'] += 1
                continue

            # Brand heuristics
            is_brand, brand_conf, brand_matches = check_brand_heuristics(user, brand_rules, args.allow_brands)
            if is_brand:
                filter_stats['brand_filtered'] += 1
                print(f"[FILTER] Brand: @{user.get('username')} (conf={brand_conf:.2f}, rules={brand_matches})", file=sys.stderr)
                continue

            # Risk flags
            risk_flags, risk_matches = check_risk_flags(user, risk_rules, allow_risk)
            if risk_flags:
                filter_stats['risk_filtered'] += 1
                print(f"[FILTER] Risk: @{user.get('username')} (flags={risk_flags}, rules={risk_matches})", file=sys.stderr)
                continue

            # Transform to schema format
            category = categories.get(user.get('username', ''), '')
            record = transform_to_schema(user, method='manual_csv', category=category,
                                       brand_rules=brand_rules, allow_brands=args.allow_brands)
            filtered_users.append(record)
            filter_stats['passed'] += 1

        # Write output
        output_path = Path(args.out)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            for record in filtered_users:
                f.write(json.dumps(record, ensure_ascii=False) + '\n')

        # Print summary
        print(f"\n[SUMMARY] Filter Statistics:", file=sys.stderr)
        print(f"  Total users: {filter_stats['total']}", file=sys.stderr)
        print(f"  Entry threshold fail: {filter_stats['entry_threshold_fail']}", file=sys.stderr)
        print(f"  Brand filtered: {filter_stats['brand_filtered']}", file=sys.stderr)
        print(f"  Risk filtered: {filter_stats['risk_filtered']}", file=sys.stderr)
        print(f"  âœ… Passed: {filter_stats['passed']}", file=sys.stderr)
        print(f"\n[OUTPUT] {args.out} ({filter_stats['passed']} records)", file=sys.stderr)

        if filter_stats['passed'] == 0:
            print("[WARN] No records passed filters - output file is empty", file=sys.stderr)
            print("[INFO] This is expected for M1 smoke test without RUBE MCP integration", file=sys.stderr)

        return 0

    except Exception as e:
        print(f"[ERROR] {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return 1


def main():
    parser = argparse.ArgumentParser(
        description="influx-harvest: Discover authors via GitHub seeds + following-graph"
    )
    subparsers = parser.add_subparsers(dest="command", required=True)

    # github-seeds subcommand
    github_parser = subparsers.add_parser(
        "github-seeds", help="Fetch GitHub org members with twitter_username"
    )
    github_parser.add_argument(
        "--orgs", required=True, help="Comma-separated GitHub org names"
    )
    github_parser.add_argument(
        "--out", required=True, help="Output JSONL file (authors)"
    )

    # following subcommand
    following_parser = subparsers.add_parser(
        "following", help="Expand via TWITTER_FOLLOWING (who seeds follow)"
    )
    following_parser.add_argument("--seeds", required=True, help="Seed authors JSONL")
    following_parser.add_argument(
        "--pages", type=int, default=2, help="Pages per seed (default: 2)"
    )
    following_parser.add_argument(
        "--out", required=True, help="Output JSONL file (expanded authors)"
    )

    # x-lists subcommand
    lists_parser = subparsers.add_parser(
        "x-lists", help="Import curated X Lists (manual CSV)"
    )
    lists_parser.add_argument(
        "--list-urls", required=True, help="CSV file with handles (handle,category,source,note)"
    )
    lists_parser.add_argument(
        "--out", required=True, help="Output JSONL file (curated authors)"
    )
    lists_parser.add_argument(
        "--min-followers", type=int, default=50000, help="Entry threshold (default: 50000)"
    )
    lists_parser.add_argument(
        "--verified-min-followers", type=int, default=30000,
        help="Lower threshold for verified accounts (default: 30000)"
    )
    lists_parser.add_argument(
        "--brand-rules", default="lists/rules/brand_heuristics.yml",
        help="Brand heuristics YAML (default: lists/rules/brand_heuristics.yml)"
    )
    lists_parser.add_argument(
        "--risk-rules", default="lists/rules/risk_terms.yml",
        help="Risk terms YAML (default: lists/rules/risk_terms.yml)"
    )
    lists_parser.add_argument(
        "--allow-brands", action="store_true",
        help="Disable brand exclusion (default: false)"
    )
    lists_parser.add_argument(
        "--allow-risk", default="",
        help="Comma-separated risk flags to allow (default: none)"
    )
    lists_parser.add_argument(
        "--prefetched-users",
        help="JSONL file with Twitter user objects (one per line) from RUBE MCP TWITTER_USER_LOOKUP_BY_USERNAMES"
    )

    args = parser.parse_args()

    if args.command == "github-seeds":
        return github_seeds(args)
    elif args.command == "following":
        return following_expansion(args)
    elif args.command == "x-lists":
        return x_lists(args)


if __name__ == "__main__":
    sys.exit(main())
